---
title: "Pythagorean_regression_2"
author: "Martin Monkman"
date: "January 7, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In [an earlier post](http://bayesball.blogspot.com/2019/01/pythagorean-win-expectation-simple.html), I looked at the relationship between run differential and wins in Major League Baseball, using Bill James' Pythagorean model.

For this post, I will pick up where I left off, and use linear regression--that workhorse of statistics, machine learning, artificial intelligence, econometrics, etc.--to look more deeply at the relationship. 

## The data

First, we'll load the packages we need. Note that [**tidyverse**](https://www.tidyverse.org/) contains multiple packages, including the graphing package **ggplot2** and the data wrangling package **dplyr**. We'll also load [**broom**](https://broom.tidyverse.org/), which facilitates manipulation of the model outputs.


```{r packages}

#tidyverse
library(tidyverse)
library(broom)

#utilities
library(datapasta)
library(feather)
library(here)

#more baseball data
library(Lahman)

```



For this analysis, we'll use the Major League Baseball data from 1961 through 2018. Since I am not a fan of repeating myself and/or recycling, I'll just use the file that I created that drives my [run scoring trends Shiny app](https://monkmanmh.shinyapps.io/MLBrunscoring_shiny/). 

The data comes from the “Lahman” baseball database, so-named after the originator and host, Sean Lahman. The website www.seanlahman.com has the current version of the database, as well as an archive of previous versions.

* The data table “Teams” is sourced from the baseballdatabank on github https://github.com/chadwickbureau/baseballdatabank)); I’ve also saved in the github repo for this app.

* There is an [R package with the Lahman database](https://github.com/cdalzell/Lahman), but it’s currently two seasons behind the source files.

![](Lahman_hex.png)



The code chunk below accesses the `Teams` table from the Lahman database, and filters out the years prior to 1962 (The first year both leagues played a 162 game schedule; in 1961, the American League added two teams and the schedule increased from 154 to 162 games and the National League followed suit in 1962.)

and adds (though the `dplyr::mutate` function) two new variables: the team's winning percentage, and using the `winexp_fun` function from the previous post, the win expectation.

```{r}

here::here()

Teams <- read_feather(here("pythagorean", "data", "Teams_merge.feather"))

Teams_sel <- Teams %>%
  filter(yearID >= 1961) %>%
  rename(RS = R.x,
         RA = RA.x) 

# win expectation function
winexp_fun <- function(RS, RA) {
  RS^2 / (RS^2 + RA^2)
}


Teams_sel <- Teams_sel %>%
  mutate(winpct = W / (W + L), 
         winexp = winexp_fun(RS, RA))

Teams_sel

```



basic plot

```{r}

we_scatterplot <- ggplot(data = Teams_sel, aes(x = winexp, y = winpct)) +
  geom_blank() +
  coord_fixed() +
  scale_x_continuous(breaks = seq(0.2, 0.8, by = 0.1),
                     limits = c(0.2, 0.8)) +
  scale_y_continuous(breaks = seq(0.2, 0.8, by = 0.1),
                     limits = c(0.2, 0.8)) 

we_scatterplot +
  geom_point()

```



#### The regression model

Let's model the relationship between the win expectation `winexp` and winning percentage `winpct`. **R** makes regression model building easy; the `lm()` function in base **R**  is all that it takes to run a linear regression, that workhorse of statistics, machine learning, artificial intelligence, econometrics, ... 

Sidebar: If you're not familiar with linear regression as a statistical method and/or how to run and interpret linear regression in R, there's an abundance of resources now available. One such is chapter 15 of Danielle Navarro's book [_Learning Statistics With R_](https://learningstatisticswithr-bookdown.netlify.com/regression.html), which gets a nod for 

* being an open on-line text, 

* relies on RStudio as the interface, 

* is clearly written with good examples, and 

* is not without a touch of levity.

With a model object assigned by the `lm()` function, we can use the `print()` and `summary()` functions let us see the key elements from the model.

Note that with the `lm()` function, the outcome (dependent) variable--the one that gets plotted on the Y axis of our chart--comes first, and the independent variable is second.


```{r}

winexp_mod <- lm(winpct ~ winexp, Teams_sel)

print(winexp_mod)

summary(winexp_mod)

```



**ggplot2** also makes it very easy to add the regression model line to the scatter plot, with the `geom_smooth()` function. Note that we have to specify the `method = lm`; the default is a [LOESS smoothing](https://en.wikipedia.org/wiki/Local_regression) curve.


```{r}

we_scatterplot +
  geom_point() +
  geom_smooth(method = lm)


```



We can use the **broom** package to get the model statistics.

```{r}

broom::tidy(winexp_mod)

```


```{r}

glance(winexp_mod)

```

```{r}

winexp_mod_tidy <- augment(winexp_mod)
head(winexp_mod_tidy)

```



The Pythagorean model explains a lot of the variation: the R-squared value of `r round(glance(winexp_mod)$r.squared, 3)`.



And just for fun, let's add a visual comparison of the "unity" line shown in red (where `winexp` == `winpct`) and the line of best fit created by the regression model (in blue). There's a slight discrepancy; it makes intuitive sense that teams with big run differentials are winning more games by bigger margins than average, and are "underperforming" relative to the Pythagorean prediction. Conversely, teams at the bottom left are overperforming Pythagorean, suggesting winning more close games. The warrants further analysis, but is a project for another day.

If we omit the `geom_point()` layer, all that gets shown are the lines. I have simply commented out that line, and added the `geom_smooth()` (as was done above).

```{r}

we_scatterplot +
#  geom_point() +
  geom_segment(aes(x = 0.25, xend = 0.75, y = 0.25, yend = 0.75), colour = "red", size = 1.5) +
  geom_smooth(method = lm, size = 1.5)

```




```{r}

final_we_scatterplot <-
we_scatterplot +
  geom_point(colour = "grey75") +
  geom_segment(aes(x = 0.25, xend = 0.75, y = 0.25, yend = 0.75), colour = "red", size = 1.5) +
  geom_smooth(method = lm, size = 1.5)

final_we_scatterplot

```



---


## ETC - future analysis possibilities

Idea: filter where .se.fit is poor (i.e. big residual), plot -- see where the 2018 Mariners sit on the list

* use `.std.resid`, the [standardized residual](http://www.r-tutor.com/elementary-statistics/simple-linear-regression/standardized-residual) 

```{r}
winexp_mod_tidy <- augment(winexp_mod)
head(winexp_mod_tidy)

winexp_mod_tidy %>%
  filter(abs(.std.resid) > 2.5)

Teams_sel_hiresid <- Teams_sel %>% 
  merge(winexp_mod_tidy) %>%
  filter(abs(.std.resid) > 2.5) %>%
  arrange(.std.resid)

  
Teams_sel_hiresid

Teams_sel_hiresid %>%
  group_by(franchID) %>%
  tally() %>%
  arrange(desc(n))

```


```{r}

we_scatterplot +
  geom_point(data = Teams_sel_hiresid, aes(x = winexp, y = winpct))



final_we_scatterplot + 
  geom_point(data = Teams_sel_hiresid, aes(x = winexp, y = winpct))


```


Idea: regression to the mean over the course of a season, via a Bayesian MCMC approach 



---

#### R pacakge references

**broom** 

* David Robinson, 2015-03-19, ["broom: a package for tidying statistical models into data frames"](https://www.r-bloggers.com/broom-a-package-for-tidying-statistical-models-into-data-frames/)

* [Introduction to broom](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)

* also https://cran.r-project.org/web/packages/broom/vignettes/broom.html 

* [broom 0.5.0](https://www.tidyverse.org/articles/2018/07/broom-0-5-0/) 



#### `ggplot2 3.0.0`

This also seemed like a good time to take [the latest version of `ggplot2` (3.0.0)](https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/) for a test run.



#### `ggeffects` package

Daniel Lüdecke, 2018-07-04, ["Marginal Effects for Regression Models in R"](https://www.r-bloggers.com/marginal-effects-for-regression-models-in-r-rstats-dataviz/)



-30-
